<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yael Balbastre </title> <meta name="author" content="Yael Balbastre"> <meta name="description" content="My research. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://balbasty.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yael</span> Balbastre </h1> <p class="desc">Newton International Fellow at University College London </p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/yael_picture-480.webp 480w,/assets/img/yael_picture-800.webp 800w,/assets/img/yael_picture-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/yael_picture.png?fb42639089bd02f824a80dad8cb86654" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="yael_picture.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>y.balbastre <i>at</i> ucl.ac.uk</p> <p><a href="https://www.ucl.ac.uk/pals/research/experimental-psychology" rel="external nofollow noopener" target="_blank">Department of Experimental Psychology</a>, <a href="https://www.ucl.ac.uk/pals/ucl-division-psychology-and-language-sciences" rel="external nofollow noopener" target="_blank">Division of Psychology and Language Sciences</a>, <a href="https://www.ucl.ac.uk" rel="external nofollow noopener" target="_blank">University College London</a></p> <p>26 Bedford Way, London WC1H 0AP</p> </div> </div> <div class="clearfix"> <p>I work on various aspects of medical image computing, with a focus on neuroimaging. I am particularly interested in generative probabilistic models, and how they can be integrated with machine learning techniques. I have worked, among other things, on Bayesian shape modelling, image segmentation and registration, and quantitative MRI. Recently, I have focused on multimodal image registration and vasculature segmentation, with the aim of building cellular-resolution atlases of the human brain.</p> <p>Before (re)joining University College London (UCL), I was a faculty member at the Massachusetts General Hospital (MGH) and Harvard Medical School (HMS). Previously, I was a postdoctoral fellow with <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> at MGH/HMS, and with <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> and <a href="https://www.fil.ion.ucl.ac.uk/team/physics-team/" rel="external nofollow noopener" target="_blank">Martina Callaghan</a> at UCL. I completed my PhD at the French Alternative Energies and Atomic Energy Commission (CEA), in the MIRCen and NeuroSpin laboratories, advised by <a href="https://jacob.cea.fr/drf/ifrancoisjacob/english/Pages/Departments/MIRCen/ResearchThemes/brain-aging.aspx" rel="external nofollow noopener" target="_blank">Thierry Delzescaux</a> and <a href="https://joliot.cea.fr/drf/joliot/en/Pages/research_entities/NeuroSpin/unati.aspx" rel="external nofollow noopener" target="_blank">Jean-Fran√ßois Mangin</a>.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 03, 2024</th> <td> Check out Etienne‚Äôs new preprint <a href="https://arxiv.org/abs/2407.01419" rel="external nofollow noopener" target="_blank">‚ÄúNeurovascular Segmentation in sOCT with Deep Learning and Synthetic Training Data‚Äù</a>! He is presenting a condensed version of this work at <a href="https://2024.midl.io" rel="external nofollow noopener" target="_blank">MIDL 2024</a> today. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 27, 2024</th> <td> Etienne‚Äôs short paper on <a href="https://arxiv.org/abs/2405.13757" rel="external nofollow noopener" target="_blank">‚Äúlabel-free and data-free vasculature segmentation‚Äù</a> was accepted at <a href="https://2024.midl.io" rel="external nofollow noopener" target="_blank">MIDL 2024</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 01, 2024</th> <td> I am starting a new adventure at University College London as a <a href="https://royalsociety.org/news/2023/10/early-career-researchers-funding-2023/" rel="external nofollow noopener" target="_blank">Newton International Fellow</a>, where I will work on <em>High-resolution spatiotemporal models of the brain across the lifespan for diagnosis and decision-making</em>. </td> </tr> <tr> <th scope="row" style="width: 20%">Feb 26, 2024</th> <td> Sean‚Äôs paper on <a href="http://arxiv.org/abs/2312.03102" rel="external nofollow noopener" target="_blank">‚ÄúFully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI‚Äù</a> was accepted at <a href="https://cvpr.thecvf.com/Conferences/2024" rel="external nofollow noopener" target="_blank">CVPR 2024</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 16, 2024</th> <td> We have a website! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CVPR</abbr> </div> <div id="young2024fully" class="col-sm-8"> <div class="title">Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI</div> <div class="author"> <a href="https://seaniyoung.com" rel="external nofollow noopener" target="_blank">Sean I Young</a><sup>‚Ä†</sup> ,¬† <em>Ya√´l Balbastre</em><sup>‚Ä†</sup> ,¬† <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> ,¬† Polina Golland , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Juan Eugenio Iglesias' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.03102" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Young_Fully_Convolutional_Slice-to-Volume_Reconstruction_for_Single-Stack_MRI_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://github.com/seannz/svr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising, current SVR methods require multiple slice stacks for accurate 3D reconstruction, leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here, we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at http://github.com/seannz/svr.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Sci.Adv.</abbr> </div> <div id="costantini2023cellular" class="col-sm-8"> <div class="title">A cellular resolution atlas of Broca‚Äôs area</div> <div class="author"> Irene Costantini<sup>‚Ä†</sup> ,¬† Leah Morgan<sup>‚Ä†</sup> ,¬† Jiarui Yang<sup>‚Ä†</sup> ,¬† <em>Yael Balbastre</em><sup>‚Ä†</sup> , and <span class="more-authors" title="click to view 35 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '35 more authors' ? 'Divya Varadarajan&lt;sup&gt;‚Ä†&lt;/sup&gt;, Luca Pesce, Marina Scardigli, Giacomo Mazzamuto, Vladislav Gavryusev, Filippo Maria Castelli, Matteo Roffilli, Ludovico Silvestri, Jessie Laffey, Sophia Raia, Merina Varghese, Bridget Wicinski, Shuaibin Chang, Ichun Anderson Chen, Hui Wang, Devani Cordero, Matthew Vera, Jackson Nolan, Kimberly Nestor, Jocelyn Mora, Juan Eugenio Iglesias, Erendira Garcia Pallares, Kathryn Evancic, Jean C. Augustinack, Morgan Fogarty, Adrian V. Dalca, Matthew P. Frosch, Caroline Magnain, Robert Frost, Andre Kouwe, Shih-Chi Chen, David A. Boas&lt;sup&gt;‚Ä°&lt;/sup&gt;, Francesco Saverio Pavone&lt;sup&gt;‚Ä°&lt;/sup&gt;, Bruce Fischl&lt;sup&gt;‚Ä°&lt;/sup&gt;, Patrick R. Hof&lt;sup&gt;‚Ä°&lt;/sup&gt;' : '35 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">35 more authors</span> </div> <div class="periodical"> <em>Science Advances</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.biorxiv.org/content/10.1101/2021.10.20.464979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">bioRxiv</a> <a href="https://www.science.org/doi/full/10.1126/sciadv.adg3844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.science.org/doi/pdf/10.1126/sciadv.adg3844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/neuroscales/biccn-broca-I46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dandiarchive.org/#/dandiset/000026/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://www.nature.com/articles/d43978-023-00159-9" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">üóû Nature Italy</a> <a href="https://www.massgeneral.org/news/press-release/researchers-develop-technology-to-tabulate-and-characterize-every-cell-in-the-human-brain" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">ü™ß PR MGH</a> <a href="https://www.mountsinai.org/about/newsroom/2023/cellular-atlas-of-a-human-brain-language-area-is-constructed-by-a-team-of-international-scientists" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">ü™ß PR Mount Sinai</a> </div> <div class="abstract hidden"> <p>Brain cells are arranged in laminar, nuclear, or columnar structures, spanning a range of scales. Here, we construct a reliable cell census in the frontal lobe of human cerebral cortex at micrometer resolution in a magnetic resonance imaging (MRI)‚Äìreferenced system using innovative imaging and analysis methodologies. MRI establishes a macroscopic reference coordinate system of laminar and cytoarchitectural boundaries. Cell counting is obtained with a digital stereological approach on the 3D reconstruction at cellular resolution from a custom-made inverted confocal light-sheet fluorescence microscope (LSFM). Mesoscale optical coherence tomography enables the registration of the distorted histological cell typing obtained with LSFM to the MRI-based atlas coordinate system. The outcome is an integrated high-resolution cellular census of Broca‚Äôs area in a human postmortem specimen, within a whole-brain reference space atlas.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MRM</abbr> </div> <div id="balbastre2022correcting" class="col-sm-8"> <div class="title">Correcting inter-scan motion artifacts in quantitative R1 mapping at 7T</div> <div class="author"> <em>Ya√´l Balbastre</em> ,¬† Ali Aghaeifar ,¬† Nad√®ge Corbin ,¬† <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'John Ashburner, Martina F Callaghan' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Magnetic Resonance in Medicine</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2108.10943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://onlinelibrary.wiley.com/doi/html/10.1002/mrm.29216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.29216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hMRI-group/hMRI-toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://blog.ismrm.org/2022/12/02/qa-with-yael-balbastre-and-martina-f-callaghan" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">üåü MRM highlights</a> </div> <div class="abstract hidden"> <p> Purpose: Inter-scan motion is a substantial source of error in R1 estimation methods based on multiple volumes, for example, variable flip angle (VFA), and can be expected to increase at 7T where B1 fields are more inhomogeneous. The established correction scheme does not translate to 7T since it requires a body coil reference. Here we introduce two alternatives that outperform the established method. Since they compute relative sensitivities they do not require body coil images.\{Theory: The proposed methods use coil-combined magnitude images to obtain the relative coil sensitivities. The first method efficiently computes the relative sensitivities via a simple ratio; the second by fitting a more sophisticated generative model.\{Methods: R1 maps were computed using the VFA approach. Multiple datasets were acquired at 3T and 7T, with and without motion between the acquisition of the VFA volumes. R1 maps were constructed without correction, with the proposed corrections, and (at 3T) with the previously established correction scheme. The effect of the greater inhomogeneity in the transmit field at 7T was also explored by acquiring B1+ maps at each position.\\Results: At 3T, the proposed methods outperform the baseline method. Inter-scan motion artifacts were also reduced at 7T. However, at 7T reproducibility only converged on that of the no motion condition if position-specific transmit field effects were also incorporated.\{Conclusion: The proposed methods simplify inter-scan motion correction of R1 maps and are applicable at both 3T and 7T, where a body coil is typically not available. The open-source code for all methods is made publicly available. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MedIA</abbr> </div> <div id="balbastre2021model" class="col-sm-8"> <div class="title">Model-based multi-parameter mapping</div> <div class="author"> <em>Ya√´l Balbastre</em> ,¬† <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,¬† Michela Azzarito ,¬† Christian Lambert , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Martina F Callaghan, John Ashburner' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Medical Image Analysis</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.01604" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S136184152100195X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S136184152100195X/pdfft?md5=e4040a3a9fc2d2accc00ef7cccb3333e&amp;pid=1-s2.0-S136184152100195X-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/balbasty/nitorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://www.miccai.org/about-miccai/awards/medical-image-analysis-best-paper-award/" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">üåü MICCAI-MedIA Best paper award</a> </div> <div class="abstract hidden"> <p>Quantitative MR imaging is increasingly favoured for its richer information content and standardised measures. However, computing quantitative parameter maps, such as those encoding longitudinal relaxation rate (R1), apparent transverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat), involves inverting a highly non-linear function. Many methods for deriving parameter maps assume perfect measurements and do not consider how noise is propagated through the estimation procedure, resulting in needlessly noisy maps. Instead, we propose a probabilistic generative (forward) model of the entire dataset, which is formulated and inverted to jointly recover (log) parameter maps with a well-defined probabilistic interpretation (e.g., maximum likelihood or maximum a posteriori). The second order optimisation we propose for model fitting achieves rapid and stable convergence thanks to a novel approximate Hessian. We demonstrate the utility of our flexible framework in the context of recovering more accurate maps from data acquired using the popular multi-parameter mapping protocol. We also show how to incorporate a joint total variation prior to further decrease the noise in the maps, noting that the probabilistic formulation allows the uncertainty on the recovered parameter maps to be estimated. Our implementation uses a PyTorch backend and benefits from GPU acceleration. It is available at https://github.com/balbasty/nitorch.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MedIA</abbr> </div> <div id="ashburner2019algorithm" class="col-sm-8"> <div class="title">An algorithm for learning shape and appearance models without annotations</div> <div class="author"> <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,¬† <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,¬† Kevin Bronik ,¬† and¬† <em>Yael Balbastre</em> </div> <div class="periodical"> <em>Medical image analysis</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/html/1807.10731" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841518305462" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841518305462/pdfft?md5=8e907152c9709f68fdd9b5feca5f1201&amp;pid=1-s2.0-S1361841518305462-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WTCN-computational-anatomy-group/Shape-Appearance-Model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. It is based on the idea that having a more accurate shape and appearance model leads to more accurate image registration, which in turn leads to a more accurate shape and appearance model. This leads naturally to an iterative scheme, which is based on a probabilistic generative model that is fit using Gauss-Newton updates within an EM-like framework. It was developed with the aim of enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications.\\The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle ‚Äúmissing data‚Äù, which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1,900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%79%62%61%6C%62%61%73%74%72%65@%6D%67%68.%68%61%72%76%61%72%64.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0001-8758-9978" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=VvO_GoYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/balbasty" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/yael-balbastre" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 Yael Balbastre. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>