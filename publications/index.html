<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Yael Balbastre </title> <meta name="author" content="Yael Balbastre"> <meta name="description" content="My research. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%AC&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://balbasty.github.io/publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yael </span> Balbastre </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="tierney2025spm" class="col-sm-8"> <div class="title">SPM 25: open source neuroimaging analysis software</div> <div class="author"> Tim M. Tierney ,  Nicholas A. Alexander ,  Nicole Labra Avila ,  <em>Yael Balbastre</em> ,  Gareth Barnes ,  Yulia Bezsudnova , and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Mikael Brudfors, Korbinian Eckstein, Guillaume Flandin, Karl Friston, Amirhossein Jafarian, Olivia S. Kowalczyk, Vladimir Litvak, Johan Medrano, Stephanie Mellor, George O’Neill, Thomas Parr, Adeel Razi, Ryan Timms, Peter Zeidman' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">14 more authors</span> </div> <div class="periodical"> <em>arXiv</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2501.12081" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2501.12081.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/spm/spm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Statistical Parametric Mapping (SPM) is an integrated set of methods for testing hypotheses about the brain’s structure and function, using data from imaging devices. These methods are implemented in an open source software package, SPM, which has been in continuous development for more than 30 years by an international community of developers. This paper reports the release of SPM 25.01, a major new version of the software that incorporates novel analysis methods, optimisations of existing methods, as well as improved practices for open science and software development.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Nat.Biomed.Eng.</abbr> </div> <div id="balbastre2025generalist" class="col-sm-8"> <div class="title">A generalist model for enhancing brain MRIs</div> <div class="author"> <em>Yael Balbastre</em> ,  and  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> </div> <div class="periodical"> <em>Nature Biomedical Engineering</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.nature.com/articles/s41551-024-01320-5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Training a model on fetal and paediatric magnetic resonance images with synthesized artefacts enhances the model’s generalization across downstream tasks and patient populations.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="hu2024learn2synth" class="col-sm-8"> <div class="title">Learn2Synth: Learning Optimal Data Synthesis Using Hypergradients</div> <div class="author"> Xiaoling Hu ,  Oula Puonti ,  <a href="https://lemon.martinos.org/" rel="external nofollow noopener" target="_blank">Juan Eugenio Iglesias</a> ,  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a><sup>‡</sup> ,  and  <em>Yael Balbastre</em><sup>‡</sup> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.16719" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2411.16719.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/HuXiaoling/Learn2Synth" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Domain randomization through synthesis is a powerful strategy to train networks that are unbiased with respect to the domain of the input images. Randomization allows networks to see a virtually infinite range of intensities and artifacts during training, thereby minimizing overfitting to appearance and maximizing generalization to unseen data. While powerful, this approach relies on the accurate tuning of a large set of hyper-parameters governing the probabilistic distribution of the synthesized images. Instead of manually tuning these parameters, we introduce Learn2Synth, a novel procedure in which synthesis parameters are learned using a small set of real labeled data. Unlike methods that impose constraints to align synthetic data with real data (e.g., contrastive or adversarial techniques), which risk misaligning the image and its label map, we tune an augmentation engine such that a segmentation network trained on synthetic data has optimal accuracy when applied to real data. This approach allows the training procedure to benefit from real labeled examples, without ever using these real examples to train the segmentation network, which avoids biasing the network towards the properties of the training set. Specifically, we develop both parametric and nonparametric strategies to augment the synthetic images, enhancing the segmentation network’s performance. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of this learning strategy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Imag.Neurosci.</abbr> </div> <div id="gopinath2024synthetic" class="col-sm-8"> <div class="title">Synthetic data in generalizable, learning-based neuroimaging</div> <div class="author"> Karthik Gopinath<sup>†</sup> ,  Andrew Hoopes<sup>†</sup> ,  Daniel C. Alexander ,  Steven E. Arnold ,  <em>Yael Balbastre</em> ,  Benjamin Billot , and <span class="more-authors" title="click to view 21 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '21 more authors' ? 'Adrià Casamitjana, You Cheng, Russ Yue Zhi Chua, Brian L. Edlow, Bruce Fischl, Harshvardhan Gazula, Malte Hoffmann, C. Dirk Keene, Seunghoi Kim, W. Taylor Kimberly, Sonia Laguna, Kathleen E. Larson, Koen Van Leemput, Oula Puonti, Livia M. Rodrigues, Matthew S. Rosen, Henry F. J. Tregidgo, Divya Varadarajan, Sean I. Young, Adrian V. Dalca&lt;sup&gt;‡&lt;/sup&gt;, Juan Eugenio Iglesias&lt;sup&gt;‡&lt;/sup&gt;' : '21 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">21 more authors</span> </div> <div class="periodical"> <em>Imaging Neuroscience</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00337/124867" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://direct.mit.edu/imag/article-pdf/doi/10.1162/imag_a_00337/2480018/imag_a_00337.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Synthetic data have emerged as an attractive option for developing machine-learning methods in human neuroimaging, particularly in magnetic resonance imaging (MRI)—a modality where image contrast depends enormously on acquisition hardware and parameters. This retrospective paper reviews a family of recently proposed methods, based on synthetic data, for generalizable machine learning in brain MRI analysis. Central to this framework is the concept of domain randomization, which involves training neural networks on a vastly diverse array of synthetically generated images with random contrast properties. This technique has enabled robust, adaptable models that are capable of handling diverse MRI contrasts, resolutions, and pathologies, while working out-of-the-box, without retraining. We have successfully applied this method to tasks such as whole-brain segmentation (SynthSeg), skull-stripping (SynthStrip), registration (SynthMorph, EasyReg), super-resolution, and MR contrast transfer (SynthSR). Beyond these applications, the paper discusses other possible use cases and future work in our methodology. Neural networks trained with synthetic data enable the analysis of clinical MRI, including large retrospective datasets, while greatly alleviating (and sometimes eliminating) the need for substantial labeled datasets, and offer enormous potential as robust tools to address various research goals.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Cereb.Cortex</abbr> </div> <div id="zeng2024segmentation" class="col-sm-8"> <div class="title">Segmentation of supragranular and infragranular layers in ultra-high resolution 7T ex vivo MRI of the human cerebral cortex</div> <div class="author"> Xiangrui Zeng ,  Oula Puonti ,  Areej Sayeed ,  Rogeny Herisse ,  Jocelyn Mora ,  Kathryn Evancic , and <span class="more-authors" title="click to view 17 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '17 more authors' ? 'Divya Varadarajan, Yael Balbastre, Irene Costantini, Marina Scardigli, Josephine Ramazzotti, Danila DiMeo, Giacomo Mazzamuto, Luca Pesce, Niamh Brady, Franco Cheli, Francesco Saverio Pavone, Patrick R. Hof, Robert Frost, Jean Augustinack, André Kouwe, Juan Eugenio Iglesias, Bruce Fischl' : '17 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">17 more authors</span> </div> <div class="periodical"> <em>Cerebral Cortex</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.biorxiv.org/content/10.1101/2023.12.06.570416" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">bioRxiv</a> <a href="https://www.biorxiv.org/content/10.1101/2023.12.06.570416v1.full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.biorxiv.org/content/biorxiv/early/2023/12/08/2023.12.06.570416.full.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Accurate labeling of specific layers in the human cerebral cortex is crucial for advancing our understanding of neurodevelopmental and neurodegenerative disorders. Building on recent advancements in ultra-high-resolution ex vivo MRI, we present a novel semi-supervised segmentation model capable of identifying supragranular and infragranular layers in ex vivo MRI with unprecedented precision. On a dataset consisting of 17 whole-hemisphere ex vivo scans at 120 mum, we propose a Multi-resolution U-Nets framework that integrates global and local structural information, achieving reliable segmentation maps of the entire hemisphere, with Dice scores over 0.8 for supra- and infragranular layers. This enables surface modeling, atlas construction, anomaly detection in disease states, and cross-modality validation while also paving the way for finer layer segmentation. Our approach offers a powerful tool for comprehensive neuroanatomical investigations and holds promise for advancing our mechanistic understanding of progression of neurodegenerative diseases.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="chollet2024neurovascular" class="col-sm-8"> <div class="title">Neurovascular Segmentation in sOCT with Deep Learning and Synthetic Training Data</div> <div class="author"> Etienne Chollet<sup>†</sup> ,  <em>Yaël Balbastre</em><sup>†</sup> ,  Chiara Mauri ,  Caroline Magnain ,  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a><sup>‡</sup> ,  and  Hui Wang<sup>‡</sup> </div> <div class="periodical"> <em>arXiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2407.01419" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2407.01419.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/EtienneChollet/oct_vesselseg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dandiarchive.org/dandiset/000722" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22z%22:%5B0.000019999999552965164%2C%22m%22%5D%2C%22y%22:%5B0.000019999999552965164%2C%22m%22%5D%2C%22x%22:%5B0.000019999999552965164%2C%22m%22%5D%7D%2C%22position%22:%5B206.36463928222656%2C735%2C767%5D%2C%22crossSectionScale%22:1.8694431659994932%2C%22projectionScale%22:2048%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://dandiarchive.s3.amazonaws.com/zarr/acffe53b-4849-4cc2-a01d-06e424896745%22%2C%22tab%22:%22rendering%22%2C%22shaderControls%22:%7B%22normalized%22:%7B%22range%22:%5B0%2C0.02%5D%7D%7D%2C%22name%22:%22image%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22zarr://https://dandiarchive.s3.amazonaws.com/zarr/89663cb1-1526-49fa-b7ec-5de02f9e3adb/%22%2C%22tab%22:%22rendering%22%2C%22selectedAlpha%22:0.22%2C%22segments%22:%5B%5D%2C%22segmentDefaultColor%22:%22#00ffee%22%2C%22name%22:%22mask%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22zarr://https://dandiarchive.s3.amazonaws.com/zarr/c7913931-1ff7-4fe8-a86d-92b31947cb31/%22%2C%22tab%22:%22rendering%22%2C%22selectedAlpha%22:1%2C%22segments%22:%5B%5D%2C%22segmentDefaultColor%22:%22#00f900%22%2C%22name%22:%22prediction%20%28maximum%20likelihood%29%22%7D%2C%7B%22type%22:%22image%22%2C%22source%22:%22zarr://https://dandiarchive.s3.amazonaws.com/zarr/e324c757-31b3-4ec6-a7e6-25f133420632/%22%2C%22tab%22:%22rendering%22%2C%22opacity%22:1%2C%22shader%22:%22#uicontrol%20invlerp%20normalized%5Cnvoid%20main%28%29%20%7B%5Cn%20%20vec4%20color%3B%5Cn%20%20color.r%20=%20normalized%28%29%3B%5Cn%20%20color.g%20=%200.0%3B%5Cn%20%20color.b%20=%200.0%3B%5Cn%20%20color.a%20=%20normalized%28%29%3B%5Cn%20%20emitRGBA%28color%29%3B%5Cn%7D%5Cn%22%2C%22name%22:%22prediction%20%28probability%29%22%2C%22visible%22:false%7D%5D%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22prediction%20%28probability%29%22%7D%2C%22layout%22:%224panel%22%2C%22layerListPanel%22:%7B%22visible%22:true%7D%7D" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">Online viewer</a> </div> <div class="abstract hidden"> <p>Microvascular anatomy is known to be involved in various neurological disorders. However, understanding these disorders is hindered by the lack of imaging modalities capable of capturing the comprehensive three-dimensional vascular network structure at microscopic resolution. With a lateral resolution of &lt;=20 \textmum and ability to reconstruct large tissue blocks up to tens of cubic centimeters, serial-section optical coherence tomography (sOCT) is well suited for this task. This method uses intrinsic optical properties to visualize the vessels and therefore does not possess a specific contrast, which complicates the extraction of accurate vascular models. The performance of traditional vessel segmentation methods is heavily degraded in the presence of substantial noise and imaging artifacts and is sensitive to domain shifts, while convolutional neural networks (CNNs) require extensive labeled data and are also sensitive the precise intensity characteristics of the data that they are trained on. Building on the emerging field of synthesis-based training, this study demonstrates a synthesis engine for neurovascular segmentation in sOCT images. Characterized by minimal priors and high variance sampling, our highly generalizable method tested on five distinct sOCT acquisitions eliminates the need for manual annotations while attaining human-level precision. Our approach comprises two phases: label synthesis and label-to-image transformation. We demonstrate the efficacy of the former by comparing it to several more realistic sets of training labels, and the latter by an ablation study of synthetic noise and artifact models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MIDL</abbr> </div> <div id="chollet2024label" class="col-sm-8"> <div class="title">A Label-Free and Data-Free Training Strategy for Vasculature Segmentation in serial sectioning OCT Data</div> <div class="author"> Etienne Chollet ,  <em>Yaël Balbastre</em> ,  Caroline Magnain ,  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> ,  and  Hui Wang </div> <div class="periodical"> <em>MIDL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.13757" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/abs/2405.13757.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Serial sectioning Optical Coherence Tomography (sOCT) is a high-throughput, label free microscopic imaging technique that is becoming increasingly popular to study post-mortem neurovasculature. Quantitative analysis of the vasculature requires highly accurate segmentation; however, sOCT has low signal-to-noise-ratio and displays a wide range of contrasts and artifacts that depend on acquisition parameters. Furthermore, labeled data is scarce and extremely time consuming to generate. Here, we leverage synthetic datasets of vessels to train a deep learning segmentation model. We construct the vessels with semi-realistic splines that simulate the vascular geometry and compare our model with realistic vascular labels generated by constrained constructive optimization. Both approaches yield similar Dice scores, although with very different false positive and false negative rates. This method addresses the complexity inherent in OCT images and paves the way for more accurate and efficient analysis of neurovascular structures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">bioRxiv</abbr> </div> <div id="casamitjana2024next" class="col-sm-8"> <div class="title">A next-generation, histological atlas of the human brain and its application to automated brain MRI segmentation</div> <div class="author"> Adrià Casamitjana ,  Matteo Mancini ,  Eleanor Robinson ,  Loïc Peter ,  Roberto Annunziata ,  Juri Althonayan , and <span class="more-authors" title="click to view 18 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '18 more authors' ? 'Shauna Crampsie, Emily Blackburn, Benjamin Billot, Alessia Atzeni, Oula Puonti, Yaël Balbastre, Peter Schmidt, James Hughes, Jean C Augustinack, Brian L Edlow, Lilla Zöllei, David L Thomas, Dorit Kliemann, Martina Bocchetta, Catherine Strand, Janice L Holton, Zane Jaunmuktane, Juan Eugenio Iglesias' : '18 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">18 more authors</span> </div> <div class="periodical"> <em>bioRxiv</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.biorxiv.org/content/10.1101/2024.02.05.579016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">bioRxiv</a> <a href="https://www.biorxiv.org/content/biorxiv/early/2024/02/06/2024.02.05.579016.full.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/acasamitjana/ERC_reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://doi.org/10.5522/04/24243835" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://github-pages.ucl.ac.uk/NextBrain/#/home" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">Online viewer</a> </div> <div class="abstract hidden"> <p>Magnetic resonance imaging (MRI) is the standard tool to image the human brain in vivo. In this domain, digital brain atlases are essential for subject-specific segmentation of anatomical regions of interest (ROIs) and spatial comparison of neuroanatomy from different subjects in a common coordinate frame. High-resolution, digital atlases derived from histology (e.g., Allen atlas [3], BigBrain [4], Julich [5]), are currently the state of the art and provide exquisite 3D cytoarchitectural maps, but lack probabilistic labels throughout the whole brain. Here we present NextBrain, a next-generation probabilistic atlas of human brain anatomy built from serial 3D histology and corresponding highly granular delineations of five whole brain hemispheres. We developed AI techniques to align and reconstruct  10,000 histological sections into coherent 3D volumes, as well as to semi-automatically trace the boundaries of 333 distinct anatomical ROIs on all these sections. Comprehensive delineation on multiple cases enabled us to build an atlas with probabilistic labels throughout the whole brain. Further, we created a companion Bayesian tool for automated segmentation of the 333 ROIs in any in vivo or ex vivo brain MRI scan using the NextBrain atlas. We showcase two applications of the atlas: automated segmentation of ultra-high-resolution ex vivo MRI and volumetric analysis of brain ageing based on  4,000 publicly available in vivo MRI scans. We publicly release the raw and aligned data (including an online visualisation tool), probabilistic atlas, and segmentation tool. By enabling researchers worldwide to analyse brain MRI scans at a superior level of granularity without manual effort or highly specific neuroanatomical knowledge, NextBrain will accelerate our quest to understand the human brain in health and disease.Competing Interest StatementThe authors have declared no competing interest.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">eLife</abbr> </div> <div id="gazula2024machine" class="col-sm-8"> <div class="title">Machine learning of dissection photographs and surface scanning for quantitative 3D neuropathology</div> <div class="author"> Harshvardhan Gazula ,  Henry F. J. Tregidgo ,  Benjamin Billot ,  <em>Yael Balbastre</em> ,  Jonathan William-Ramirez ,  Rogeny Herisse , and <span class="more-authors" title="click to view 20 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '20 more authors' ? 'Lucas J Deden-Binder, Adrià Casamitjana, Erica J. Melief, Caitlin S. Latimer, Mitchell D. Kilgore, Mark Montine, Eleanor Robinson, Emily Blackburn, Michael S. Marshall, Theresa R. Connors, Derek H. Oakley, Matthew P. Frosch, Sean I. Young, Koen Van Leemput, Adrian V. Dalca, Bruce Fischl, Christine L. Mac Donald, C. Dirk Keene, Bradley T. Hyman, Juan Eugenio Iglesias' : '20 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">20 more authors</span> </div> <div class="periodical"> <em>eLife</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.biorxiv.org/content/10.1101/2023.06.08.544050" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">bioRxiv</a> <a href="https://elifesciences.org/articles/91398" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.biorxiv.org/content/10.1101/2023.06.08.544050v3.full.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/MGH-LEMoN/photo-reconstruction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present open-source tools for 3D analysis of photographs of dissected slices of human brains, which are routinely acquired in brain banks but seldom used for quantitative analysis. Our tools can: (i) 3D reconstruct a volume from the photographs and, optionally, a surface scan; and (ii) produce a high-resolution 3D segmentation into 11 brain regions per hemisphere (22 in total), independently of the slice thickness. Our tools can be used as a substitute for ex vivo magnetic resonance imaging (MRI), which requires access to an MRI scanner, ex vivo scanning expertise, and considerable financial resources. We tested our tools on synthetic and real data from two NIH Alzheimer’s Disease Research Centers. The results show that our methodology yields accurate 3D reconstructions, segmentations, and volumetric measurements that are highly correlated to those from MRI. Our method also detects expected differences between post mortem confirmed Alzheimer’s disease cases and controls. The tools are available in our widespread neuroimaging suite “FreeSurfer” (https://surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CVPR</abbr> </div> <div id="young2024fully" class="col-sm-8"> <div class="title">Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI</div> <div class="author"> <a href="https://seaniyoung.com" rel="external nofollow noopener" target="_blank">Sean I Young</a><sup>†</sup> ,  <em>Yaël Balbastre</em><sup>†</sup> ,  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> ,  Polina Golland ,  and  <a href="https://lemon.martinos.org/" rel="external nofollow noopener" target="_blank">Juan Eugenio Iglesias</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.03102" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Young_Fully_Convolutional_Slice-to-Volume_Reconstruction_for_Single-Stack_MRI_CVPR_2024_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://github.com/seannz/svr" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising, current SVR methods require multiple slice stacks for accurate 3D reconstruction, leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here, we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at http://github.com/seannz/svr.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">CBM @ MICCAI</abbr> </div> <div id="french2024diffeomorphic" class="col-sm-8"> <div class="title">Diffeomorphic Multi-resolution Deep Learning Registration for Applications in Breast MRI</div> <div class="author"> Matthew G. French ,  Gonzalo D. Maso Talou ,  Thiranja P. Babarenda Gamage ,  Martyn P. Nash ,  Poul M. F. Nielsen ,  Anthony J. Doyle , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Juan Eugenio Iglesias, Yaël Balbastre, Sean I. Young' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Computational Biomechanics for Medicine</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.13777" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/2309.13777.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In breast surgical planning, accurate registration of MR images across patient positions has the potential to improve the localisation of tumours during breast cancer treatment. While learning-based registration methods have recently become the state-of-the-art approach for most medical image registration tasks, these methods have yet to make inroads into breast image registration due to certain difficulties-the lack of rich texture information in breast MR images and the need for the deformations to be diffeomophic. In this work, we propose learning strategies for breast MR image registration that are amenable to diffeomorphic constraints, together with early experimental results from in-silico and in-vivo experiments. One key contribution of this work is a registration network which produces superior registration outcomes for breast images in addition to providing diffeomorphic guarantees.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Sci.Adv.</abbr> </div> <div id="costantini2023cellular" class="col-sm-8"> <div class="title">A cellular resolution atlas of Broca’s area</div> <div class="author"> Irene Costantini<sup>†</sup> ,  Leah Morgan<sup>†</sup> ,  Jiarui Yang<sup>†</sup> ,  <em>Yael Balbastre</em><sup>†</sup> ,  Divya Varadarajan<sup>†</sup> ,  Luca Pesce , and <span class="more-authors" title="click to view 33 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '33 more authors' ? 'Marina Scardigli, Giacomo Mazzamuto, Vladislav Gavryusev, Filippo Maria Castelli, Matteo Roffilli, Ludovico Silvestri, Jessie Laffey, Sophia Raia, Merina Varghese, Bridget Wicinski, Shuaibin Chang, Ichun Anderson Chen, Hui Wang, Devani Cordero, Matthew Vera, Jackson Nolan, Kimberly Nestor, Jocelyn Mora, Juan Eugenio Iglesias, Erendira Garcia Pallares, Kathryn Evancic, Jean C. Augustinack, Morgan Fogarty, Adrian V. Dalca, Matthew P. Frosch, Caroline Magnain, Robert Frost, Andre Kouwe, Shih-Chi Chen, David A. Boas&lt;sup&gt;‡&lt;/sup&gt;, Francesco Saverio Pavone&lt;sup&gt;‡&lt;/sup&gt;, Bruce Fischl&lt;sup&gt;‡&lt;/sup&gt;, Patrick R. Hof&lt;sup&gt;‡&lt;/sup&gt;' : '33 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">33 more authors</span> </div> <div class="periodical"> <em>Science Advances</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.biorxiv.org/content/10.1101/2021.10.20.464979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">bioRxiv</a> <a href="https://www.science.org/doi/full/10.1126/sciadv.adg3844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.science.org/doi/pdf/10.1126/sciadv.adg3844" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/neuroscales/biccn-broca-I46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://dandiarchive.org/#/dandiset/000026/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> <a href="https://www.nature.com/articles/d43978-023-00159-9" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🗞 Nature Italy</a> <a href="https://www.massgeneral.org/news/press-release/researchers-develop-technology-to-tabulate-and-characterize-every-cell-in-the-human-brain" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🪧 PR MGH</a> <a href="https://www.mountsinai.org/about/newsroom/2023/cellular-atlas-of-a-human-brain-language-area-is-constructed-by-a-team-of-international-scientists" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🪧 PR Mount Sinai</a> </div> <div class="abstract hidden"> <p>Brain cells are arranged in laminar, nuclear, or columnar structures, spanning a range of scales. Here, we construct a reliable cell census in the frontal lobe of human cerebral cortex at micrometer resolution in a magnetic resonance imaging (MRI)–referenced system using innovative imaging and analysis methodologies. MRI establishes a macroscopic reference coordinate system of laminar and cytoarchitectural boundaries. Cell counting is obtained with a digital stereological approach on the 3D reconstruction at cellular resolution from a custom-made inverted confocal light-sheet fluorescence microscope (LSFM). Mesoscale optical coherence tomography enables the registration of the distorted histological cell typing obtained with LSFM to the MRI-based atlas coordinate system. The outcome is an integrated high-resolution cellular census of Broca’s area in a human postmortem specimen, within a whole-brain reference space atlas.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Sci.Adv.</abbr> </div> <div id="iglesias2023synthsr" class="col-sm-8"> <div class="title">SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry</div> <div class="author"> <a href="https://lemon.martinos.org/" rel="external nofollow noopener" target="_blank">Juan E Iglesias</a> ,  Benjamin Billot ,  <em>Yaël Balbastre</em> ,  Colin Magdamo ,  Steven E Arnold ,  Sudeshna Das , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Brian L Edlow, Daniel C Alexander, Polina Golland, Bruce Fischl' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Science advances</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.science.org/doi/full/10.1126/sciadv.add3607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.science.org/doi/pdf/10.1126/sciadv.add3607" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/BBillot/SynthSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Every year, millions of brain magnetic resonance imaging (MRI) scans are acquired in hospitals across the world. These have the potential to revolutionize our understanding of many neurological diseases, but their morphometric analysis has not yet been possible due to their anisotropic resolution. We present an artificial intelligence technique, “SynthSR,” that takes clinical brain MRI scans with any MR contrast (T1, T2, etc.), orientation (axial/coronal/sagittal), and resolution and turns them into high-resolution T1 scans that are usable by virtually all existing human neuroimaging tools. We present results on segmentation, registration, and atlasing of &gt;10,000 scans of controls and patients with brain tumors, strokes, and Alzheimer’s disease. SynthSR yields morphometric results that are very highly correlated with what one would have obtained with high-resolution T1 scans. SynthSR allows sample sizes that have the potential to overcome the power limitations of prospective research studies and shed new light on the healthy and diseased human brain.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IEEE TMI</abbr> </div> <div id="hering2022learn2reg" class="col-sm-8"> <div class="title">Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning</div> <div class="author"> Alessa Hering ,  Lasse Hansen ,  Tony C. W. Mok ,  Albert C. S. Chung ,  Hanna Siebert ,  Stephanie Häger , and <span class="more-authors" title="click to view 47 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '47 more authors' ? 'Annkristin Lange, Sven Kuckertz, Stefan Heldmann, Wei Shao, Sulaiman Vesal, Mirabela Rusu, Geoffrey Sonn, Théo Estienne, Maria Vakalopoulou, Luyi Han, Yunzhi Huang, Pew-Thian Yap, Mikael Brudfors, Yaël Balbastre, Samuel Joutard, Marc Modat, Gal Lifshitz, Dan Raviv, Jinxin Lv, Qiang Li, Vincent Jaouen, Dimitris Visvikis, Constance Fourcade, Mathieu Rubeaux, Wentao Pan, Zhe Xu, Bailiang Jian, Francesca De Benetti, Marek Wodzinski, Niklas Gunnarsson, Jens Sjölund, Daniel Grzech, Huaqi Qiu, Zeju Li, Alexander Thorley, Jinming Duan, Christoph Großbröhmer, Andrew Hoopes, Ingerid Reinertsen, Yiming Xiao, Bennett Landman, Yuankai Huo, Keelin Murphy, Nikolas Lessmann, Bram Ginneken, Adrian V. Dalca, Mattias P. Heinrich' : '47 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">47 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Medical Imaging</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2112.04489" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://ieeexplore.ieee.org/abstract/document/9925717" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://ieeexplore.ieee.org/iel7/42/4359023/09925717.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Image registration is a fundamental medical image analysis task, and a wide variety of approaches have been proposed. However, only a few studies have comprehensively compared medical image registration approaches on a wide range of clinically relevant tasks. This limits the development of registration methods, the adoption of research advances into practice, and a fair benchmark across competing approaches. The Learn2Reg challenge addresses these limitations by providing a multi-task medical image registration data set for comprehensive characterisation of deformable registration algorithms. A continuous evaluation will be possible at https://learn2reg.grand-challenge.org. Learn2Reg covers a wide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound, CT, MR), availability of annotations, as well as intra- and inter-patient registration evaluation. We established an easily accessible framework for training and validation of 3D registration methods, which enabled the compilation of results of over 65 individual method submissions from more than 20 unique teams. We used a complementary set of metrics, including robustness, accuracy, plausibility, and runtime, enabling unique insight into the current state-of-the-art of medical image registration. This paper describes datasets, tasks, evaluation methods and results of the challenge, as well as results of further analysis of transferability to new datasets, the importance of label supervision, and resulting bias. While no single approach worked best across all tasks, many methodological aspects could be identified that push the performance of medical image registration to new state-of-the-art performance. Furthermore, we demystified the common belief that conventional registration methods have to be much slower than deep-learning-based methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MIUA</abbr> </div> <div id="brudfors2022fitting" class="col-sm-8"> <div class="title">Fitting Segmentation Networks on Varying Image Resolutions Using Splatting</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yaël Balbastre</em> ,  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,  Geraint Rees ,  Parashkev Nachev ,  Sébastien Ourselin , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'M Jorge Cardoso' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Annual Conference on Medical Image Understanding and Analysis</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2206.06445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-031-12053-4_21" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2206.06445" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/balbasty/nitorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Data used in image segmentation are not always defined on the same grid. This is particularly true for medical images, where the resolution, field-of-view and orientation can differ across channels and subjects. Images and labels are therefore commonly resampled onto the same grid, as a pre-processing step. However, the resampling operation introduces partial volume effects and blurring, thereby changing the effective resolution and reducing the contrast between structures. In this paper we propose a splat layer, which automatically handles resolution mismatches in the input data. This layer pushes each image onto a mean space where the forward pass is performed. As the splat operator is the adjoint to the resampling operator, the mean-space prediction can be pulled back to the native label space, where the loss function is computed. Thus, the need for explicit resolution adjustment using interpolation is removed. We show on two publicly available datasets, with simulated and real multi-modal magnetic resonance images, that this model improves segmentation results compared to resampling as a pre-processing step.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">WBIR</abbr> </div> <div id="young2022superwarp" class="col-sm-8"> <div class="title">SuperWarp: Supervised Learning and Warping on U-Net for Invariant Subvoxel-Precise Registration</div> <div class="author"> <a href="https://seaniyoung.com" rel="external nofollow noopener" target="_blank">Sean I Young</a> ,  <em>Yaël Balbastre</em> ,  Adrian V Dalca ,  William M Wells ,  <a href="https://lemon.martinos.org/" rel="external nofollow noopener" target="_blank">Juan Eugenio Iglesias</a> ,  and  <a href="https://www.martinos.org/investigator/bruce-fischl/" rel="external nofollow noopener" target="_blank">Bruce Fischl</a> </div> <div class="periodical"> <em>In International Workshop on Biomedical Image Registration</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2205.07399" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9645132/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2205.07399.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105">Oral presentation</a> </div> <div class="abstract hidden"> <p>In recent years, learning-based image registration methods have gradually moved away from direct supervision with target warps to instead use self-supervision, with excellent results in several registration benchmarks. These approaches utilize a loss function that penalizes the intensity differences between the fixed and moving images, along with a suitable regularizer on the deformation. In this paper, we argue that the relative failure of supervised registration approaches can in part be blamed on the use of regular U-Nets, which are jointly tasked with feature extraction, feature matching, and estimation of deformation. We introduce one simple but crucial modification to the U-Net that disentangles feature extraction and matching from deformation prediction, allowing the U-Net to warp the features, across levels, as the deformation field is evolved. With this modification, direct supervision using target warps begins to outperform self-supervision approaches that require segmentations, presenting new directions for registration when images do not have segmentations. We hope that our findings in this preliminary workshop paper will re-ignite research interest in supervised image registration techniques.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MRM</abbr> </div> <div id="balbastre2022correcting" class="col-sm-8"> <div class="title">Correcting inter-scan motion artifacts in quantitative R1 mapping at 7T</div> <div class="author"> <em>Yaël Balbastre</em> ,  Ali Aghaeifar ,  Nadège Corbin ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,  and  Martina F Callaghan </div> <div class="periodical"> <em>Magnetic Resonance in Medicine</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2108.10943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://onlinelibrary.wiley.com/doi/html/10.1002/mrm.29216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.29216" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/hMRI-group/hMRI-toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://blog.ismrm.org/2022/12/02/qa-with-yael-balbastre-and-martina-f-callaghan" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🌟 MRM highlights</a> </div> <div class="abstract hidden"> <p> Purpose: Inter-scan motion is a substantial source of error in R1 estimation methods based on multiple volumes, for example, variable flip angle (VFA), and can be expected to increase at 7T where B1 fields are more inhomogeneous. The established correction scheme does not translate to 7T since it requires a body coil reference. Here we introduce two alternatives that outperform the established method. Since they compute relative sensitivities they do not require body coil images.<br><br> Theory: The proposed methods use coil-combined magnitude images to obtain the relative coil sensitivities. The first method efficiently computes the relative sensitivities via a simple ratio; the second by fitting a more sophisticated generative model.<br><br> Methods: R1 maps were computed using the VFA approach. Multiple datasets were acquired at 3T and 7T, with and without motion between the acquisition of the VFA volumes. R1 maps were constructed without correction, with the proposed corrections, and (at 3T) with the previously established correction scheme. The effect of the greater inhomogeneity in the transmit field at 7T was also explored by acquiring B1+ maps at each position.<br><br>Results: At 3T, the proposed methods outperform the baseline method. Inter-scan motion artifacts were also reduced at 7T. However, at 7T reproducibility only converged on that of the no motion condition if position-specific transmit field effects were also incorporated.<br><br> Conclusion: The proposed methods simplify inter-scan motion correction of R1 maps and are applicable at both 3T and 7T, where a body coil is typically not available. The open-source code for all methods is made publicly available. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IEEE TBE</abbr> </div> <div id="yang2022volumetric" class="col-sm-8"> <div class="title">Volumetric characterization of microvasculature in ex vivo human brain samples by serial sectioning optical coherence tomography</div> <div class="author"> Jiarui Yang ,  Shuaibin Chang ,  Ichun Anderson Chen ,  Sreekanth Kura ,  Grace A. Rosen ,  Nicole A. Saltiel , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Bertrand R. Huber, Divya Varadarajan, Yael Balbastre, Caroline Magnain, Shih-chi Chen, Bruce Fischl, Ann C. McKee, David A. Boas, Hui Wang' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>IEEE Transactions on Biomedical Engineering</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9888394/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9888394/pdf/nihms-1852765.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Objective: Serial sectioning optical coherence tomography (OCT) enables accurate volumetric reconstruction of several cubic centimeters of human brain samples. We aimed to identify anatomical features of the ex vivo human brain, such as intraparenchymal blood vessels and axonal fiber bundles, from the OCT data in 3D, using intrinsic optical contrast.<br><br> Methods: We developed an automatic processing pipeline to enable characterization of the intraparenchymal microvascular network in human brain samples.<br><br> Results: We demonstrated the automatic extraction of the vessels down to a 20 μm in diameter using a filtering strategy followed by a graphing representation and characterization of the geometrical properties of microvascular network in 3D. We also showed the ability to extend this processing strategy to extract axonal fiber bundles from the volumetric OCT image.<br><br> Conclusion: This method provides a viable tool for quantitative characterization of volumetric microvascular network as well as the axonal bundle properties in normal and pathological tissues of the ex vivo human brain. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Front.Neurosci.</abbr> </div> <div id="yan2022factorisation" class="col-sm-8"> <div class="title">Factorisation-based image labelling</div> <div class="author"> Yu Yan ,  <em>Yaël Balbastre</em> ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2111.10326" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2021.818604/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2021.818604/pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WCHN/Label-Training" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Segmentation of brain magnetic resonance images (MRI) into anatomical regions is a useful task in neuroimaging. Manual annotation is time consuming and expensive, so having a fully automated and general purpose brain segmentation algorithm is highly desirable. To this end, we propose a patched-based labell propagation approach based on a generative model with latent variables. Once trained, our Factorisation-based Image Labelling (FIL) model is able to label target images with a variety of image contrasts. We compare the effectiveness of our proposed model against the state-of-the-art using data from the MICCAI 2012 Grand Challenge and Workshop on Multi-Atlas Labelling. As our approach is intended to be general purpose, we also assess how well it can handle domain shift by labelling images of the same subjects acquired with different MR contrasts.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MedIA</abbr> </div> <div id="balbastre2021model" class="col-sm-8"> <div class="title">Model-based multi-parameter mapping</div> <div class="author"> <em>Yaël Balbastre</em> ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  Michela Azzarito ,  Christian Lambert ,  Martina F Callaghan ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>Medical Image Analysis</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2102.01604" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S136184152100195X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S136184152100195X/pdfft?md5=e4040a3a9fc2d2accc00ef7cccb3333e&amp;pid=1-s2.0-S136184152100195X-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/balbasty/nitorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="http://www.miccai.org/about-miccai/awards/medical-image-analysis-best-paper-award/" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🌟 MICCAI-MedIA Best paper award</a> </div> <div class="abstract hidden"> <p>Quantitative MR imaging is increasingly favoured for its richer information content and standardised measures. However, computing quantitative parameter maps, such as those encoding longitudinal relaxation rate (R1), apparent transverse relaxation rate (R2*) or magnetisation-transfer saturation (MTsat), involves inverting a highly non-linear function. Many methods for deriving parameter maps assume perfect measurements and do not consider how noise is propagated through the estimation procedure, resulting in needlessly noisy maps. Instead, we propose a probabilistic generative (forward) model of the entire dataset, which is formulated and inverted to jointly recover (log) parameter maps with a well-defined probabilistic interpretation (e.g., maximum likelihood or maximum a posteriori). The second order optimisation we propose for model fitting achieves rapid and stable convergence thanks to a novel approximate Hessian. We demonstrate the utility of our flexible framework in the context of recovering more accurate maps from data acquired using the popular multi-parameter mapping protocol. We also show how to incorporate a joint total variation prior to further decrease the noise in the maps, noting that the probabilistic formulation allows the uncertainty on the recovered parameter maps to be estimated. Our implementation uses a PyTorch backend and benefits from GPU acceleration. It is available at https://github.com/balbasty/nitorch.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MIDL</abbr> </div> <div id="brudfors2021mrf" class="col-sm-8"> <div class="title">An MRF-UNet product of experts for image segmentation</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yaël Balbastre</em> ,  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,  Geraint Rees ,  Parashkev Nachev ,  Sébastien Ourselin , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'M Jorge Cardoso' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In Medical Imaging with Deep Learning</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2104.05495" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.mlr.press/v143/brudfors21a" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.mlr.press/v143/brudfors21a/brudfors21a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/balbasty/nitorch" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While convolutional neural networks (CNNs) trained by back-propagation have seen unprecedented success at semantic segmentation tasks, they are known to struggle on out-of-distribution data. Markov random fields (MRFs) on the other hand, encode simpler distributions over labels that, although less flexible than UNets, are less prone to over-fitting. In this paper, we propose to fuse both strategies by computing the product of distributions of a UNet and an MRF. As this product is intractable, we solve for an approximate distribution using an iterative mean-field approach. The resulting MRF-UNet is trained jointly by back-propagation. Compared to other works using conditional random fields (CRFs), the MRF has no dependency on the imaging data, which should allow for less over-fitting. We show on 3D neuroimaging data that this novel network improves generalisation to out-of-distribution samples. Furthermore, it allows the overall number of parameters to be reduced while preserving high accuracy. These results suggest that a classic MRF smoothness prior can allow for less over-fitting when principally integrated into a CNN model. Our implementation is available at https://github.com/balbasty/nitorch.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Neuroimage</abbr> </div> <div id="iglesias2021joint" class="col-sm-8"> <div class="title">Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast</div> <div class="author"> <a href="https://lemon.martinos.org/" rel="external nofollow noopener" target="_blank">Juan Eugenio Iglesias</a> ,  Benjamin Billot ,  <em>Yaël Balbastre</em> ,  Azadeh Tabari ,  John Conklin ,  R. Gilberto González , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Daniel C. Alexander, Polina Golland, Brian L. Edlow, Bruce Fischl' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Neuroimage</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2012.13340" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811921004833" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811921004833/pdfft?md5=e513287663c1f7a9086bc8f55704b412&amp;pid=1-s2.0-S1053811921004833-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/BBillot/SynthSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well-typically requiring T1-weighted images (e.g., MP-RAGE scans). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing in clinical settings every year. In turn, the inability to quantitatively analyze these scans hinders the adoption of quantitative neuro imaging in healthcare, and also precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in convolutional neural networks (CNNs) are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the specific combination of contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols - even within sites. In this article, we present SynthSR, a method to train a CNN that receives one or more scans with spaced slices, acquired with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, beyond rigid coregistration of the input scans. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution real images of the input contrasts. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at https://github.com/BBillot/SynthSR.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">HBM</abbr> </div> <div id="azzarito2021simultaneous" class="col-sm-8"> <div class="title">Simultaneous voxel-wise analysis of brain and spinal cord morphometry and microstructure within the SPM framework</div> <div class="author"> Michela Azzarito ,  Sreenath P Kyathanahally ,  <em>Yaël Balbastre</em> ,  Maryam Seif ,  Claudia Blaiotta ,  Martina F Callaghan , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'John Ashburner, Patrick Freund' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Human brain mapping</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/hbm.25218" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.25218" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>To validate a simultaneous analysis tool for the brain and cervical cord embedded in the statistical parametric mapping (SPM) framework, we compared trauma‐induced macro‐ and microstructural changes in spinal cord injury (SCI) patients to controls. The findings were compared with results obtained from existing processing tools that assess the brain and spinal cord separately. A probabilistic brain‐spinal cord template (BSC) was generated using a generative semi‐supervised modelling approach. The template was incorporated into the pre‐processing pipeline of voxel‐based morphometry and voxel‐based quantification analyses in SPM. This approach was validated on T1‐weighted scans and multiparameter maps, by assessing trauma‐induced changes in SCI patients relative to controls and comparing the findings with the outcome from existing analytical tools. Consistency of the MRI measures was assessed using intraclass correlation coefficients (ICC). The SPM approach using the BSC template revealed trauma‐induced changes across the sensorimotor system in the cord and brain in SCI patients. These changes were confirmed with established approaches covering brain or cord, separately. The ICC in the brain was high within regions of interest, such as the sensorimotor cortices, corticospinal tracts and thalamus. The simultaneous voxel‐wise analysis of brain and cervical spinal cord was performed in a unique SPM‐based framework incorporating pre‐processing and statistical analysis in the same environment. Validation based on a SCI cohort demonstrated that the new processing approach based on the brain and cord is comparable to available processing tools, while offering the advantage of performing the analysis simultaneously across the neuraxis.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">JCBFM</abbr> </div> <div id="van2020assessment" class="col-sm-8"> <div class="title">Assessment of simplified methods for quantification of [18F]-DPA-714 using 3D whole-brain TSPO immunohistochemistry in a non-human primate</div> <div class="author"> Nadja Van Camp ,  <em>Yaël Balbastre</em><sup>*</sup> ,  Anne-Sophie Herard<sup>*</sup> ,  Sonia Lavisse ,  Clovis Tauber ,  Catriona Wimberley , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Martine Guillermier, Aurélie Berniard, Pauline Gipchtein, Caroline Jan, Romina Aron Badin, Thierry Delzescaux, Philippe Hantraye, Gilles Bonvento' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Journal of Cerebral Blood Flow <br>&amp; Metabolism</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://journals.sagepub.com/doi/full/10.1177/0271678X19859034" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://journals.sagepub.com/doi/pdf/10.1177/0271678X19859034" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The 18 kDa translocator protein (TSPO) is the main molecular target to image neuroinflammation by positron emission tomography (PET). However, TSPO-PET quantification is complex and none of the kinetic modelling approaches has been validated using a voxel-by-voxel comparison of TSPO-PET data with the actual TSPO levels of expression. Here, we present a single case study of binary classification of in vivo PET data to evaluate the statistical performance of different TSPO-PET quantification methods. To that end, we induced a localized and adjustable increase of TSPO levels in a non-human primate brain through a viral-vector strategy. We then performed a voxel-wise comparison of the different TSPO-PET quantification approaches providing parametric [18F]-DPA-714 PET images, with co-registered in vitro three-dimensional TSPO immunohistochemistry (3D-IHC) data. A data matrix was extracted from each brain hemisphere, containing the TSPO-IHC and TSPO-PET data for each voxel position. Each voxel was then classified as false or true, positive or negative after comparison of the TSPO-PET measure to the reference 3D-IHC method. Finally, receiver operating characteristic curves (ROC) were calculated for each TSPO-PET quantification method. Our results show that standard uptake value ratios using cerebellum as a reference region (SUVCBL) has the most optimal ROC score amongst all non-invasive approaches.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Cell Metab.</abbr> </div> <div id="le2020impairment" class="col-sm-8"> <div class="title">Impairment of glycolysis-derived l-serine production in astrocytes contributes to cognitive deficits in Alzheimer’s disease</div> <div class="author"> Juliette Le Douce<sup>†</sup> ,  Marianne Maugard<sup>†</sup> ,  Julien Veran<sup>†</sup> ,  Marco Matos<sup>†</sup> ,  Pierrick Jégo<sup>*</sup> ,  Pierre-Antoine Vigneron<sup>*</sup> , and <span class="more-authors" title="click to view 30 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '30 more authors' ? 'Emilie Faivre&lt;sup&gt;*&lt;/sup&gt;, Xavier Toussay, Michel Vandenberghe, Yaël Balbastre, Juliette Piquet, Elvire Guiot, Nguyet Thuy Tran, Myriam Taverna, Stéphane Marinesco, Ayumi Koyanagi, Shigeki Furuya, Mylène Gaudin-Guérif, Sébastien Goutal, Aurélie Ghettas, Alain Pruvost, Alexis-Pierre Bemelmans, Marie-Claude Gaillard, Karine Cambon, Lev Stimmer, Véronique Sazdovitch, Charles Duyckaerts, Graham Knott, Anne-Sophie Hérard, Thierry Delzescaux, Philippe Hantraye, Emmanuel Brouillet, Bruno Cauli, Stéphane H.R. Oliet, Aude Panatier&lt;sup&gt;‡&lt;/sup&gt;, Gilles Bonvento&lt;sup&gt;‡&lt;/sup&gt;' : '30 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">30 more authors</span> </div> <div class="periodical"> <em>Cell metabolism</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.cell.com/cell-metabolism/fulltext/S1550-4131(20)30063-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.cell.com/cell-metabolism/pdf/S1550-4131(20)30063-2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Alteration of brain aerobic glycolysis is often observed early in the course of Alzheimer’s disease (AD). Whether and how such metabolic dysregulation contributes to both synaptic plasticity and behavioral deficits in AD is not known. Here, we show that the astrocytic l-serine biosynthesis pathway, which branches from glycolysis, is impaired in young AD mice and in AD patients. l-serine is the precursor of d-serine, a co-agonist of synaptic NMDA receptors (NMDARs) required for synaptic plasticity. Accordingly, AD mice display a lower occupancy of the NMDAR co-agonist site as well as synaptic and behavioral deficits. Similar deficits are observed following inactivation of the l-serine synthetic pathway in hippocampal astrocytes, supporting the key role of astrocytic l-serine. Supplementation with l-serine in the diet prevents both synaptic and behavioral deficits in AD mice. Our findings reveal that astrocytic glycolysis controls cognitive functions and suggest oral l-serine as a ready-to-use therapy for AD.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI</abbr> </div> <div id="brudfors2020flexible" class="col-sm-8"> <div class="title">Flexible Bayesian modelling for nonlinear image registration</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a><sup>†</sup> ,  <em>Yaël Balbastre</em><sup>†</sup> ,  Guillaume Flandin ,  Parashkev Nachev ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part III 23</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.02338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_25" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2006.02338" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WTCN-computational-anatomy-group/mb" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We describe a diffeomorphic registration algorithm that allows groups of images to be accurately aligned to a common space, which we intend to incorporate into the SPM software. The idea is to perform inference in a probabilistic graphical model that accounts for variability in both shape and appearance. The resulting framework is general and entirely unsupervised. The model is evaluated at inter-subject registration of 3D human brain scans. Here, the main modeling assumption is that individual anatomies can be generated by deforming a latent ’average’ brain. The method is agnostic to imaging modality and can be applied with no prior processing. We evaluate the algorithm using freely available, manually labelled datasets. In this validation we achieve state-of-the-art results, within reasonable runtimes, against previous state-of-the-art widely used, inter-subject registration algorithms. On the unprocessed dataset, the increase in overlap score is over 17%. These results demonstrate the benefits of using informative computational anatomy frameworks for nonlinear registration.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI</abbr> </div> <div id="balbastre2020joint" class="col-sm-8"> <div class="title">Joint total variation ESTATICS for robust multi-parameter mapping</div> <div class="author"> <em>Yaël Balbastre</em> ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  Michela Azzarito ,  Christian Lambert ,  Martina F Callaghan ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II 23</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.14247" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-59713-9_6" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2005.14247" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/balbasty/physics-toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105">Early accept</a> </div> <div class="abstract hidden"> <p>Quantitative magnetic resonance imaging (qMRI) derives tissue-specific parameters – such as the apparent transverse relaxation rate R2*, the longitudinal relaxation rate R1 and the magnetisation transfer saturation – that can be compared across sites and scanners and carry important information about the underlying microstructure. The multi-parameter mapping (MPM) protocol takes advantage of multi-echo acquisitions with variable flip angles to extract these parameters in a clinically acceptable scan time. In this context, ESTATICS performs a joint loglinear fit of multiple echo series to extract R2* and multiple extrapolated intercepts, thereby improving robustness to motion and decreasing the variance of the estimators. In this paper, we extend this model in two ways: (1) by introducing a joint total variation (JTV) prior on the intercepts and decay, and (2) by deriving a nonlinear maximum \empha posteriori estimate. We evaluated the proposed algorithm by predicting left-out echoes in a rich single-subject dataset. In this validation, we outperformed other state-of-the-art methods and additionally showed that the proposed approach greatly reduces the variance of the estimated maps, without introducing bias.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MIUA</abbr> </div> <div id="brudfors2020groupwise" class="col-sm-8"> <div class="title">Groupwise Multimodal Image Registration using Joint Total Variation</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yaël Balbastre</em> ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Medical Image Understanding and Analysis: 24th Annual Conference, MIUA 2020, Oxford, UK, July 15-17, 2020, Proceedings 24</em> , 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2005.02933" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-52791-4_15" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2005.02933" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/brudfors/coregistration-njtv" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In medical imaging it is common practice to acquire a wide range of modalities (MRI, CT, PET, etc.), to highlight different structures or pathologies. As patient movement between scans or scanning session is unavoidable, registration is often an essential step before any subsequent image analysis. In this paper, we introduce a cost function based on joint total variation for such multimodal image registration. This cost function has the advantage of enabling principled, groupwise alignment of multiple images, whilst being insensitive to strong intensity non-uniformities. We evaluate our algorithm on rigidly aligning both simulated and real 3D brain scans. This validation shows robustness to strong intensity non-uniformities and low registration errors for CT/PET to MRI alignment. Our implementation is publicly available at https://github.com/brudfors/coregistration-njtv.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Front.Neuroanat.</abbr> </div> <div id="you2019automated" class="col-sm-8"> <div class="title">Automated individualization of size-varying and touching neurons in macaque cerebral microscopic images</div> <div class="author"> Zhenzhen You ,  <em>Yaël Balbastre</em> ,  Clément Bouvier ,  Anne-Sophie Hérard ,  Pauline Gipchtein ,  Philippe Hantraye , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Caroline Jan, Nicolas Souedet, Thierry Delzescaux' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>Frontiers in Neuroanatomy</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.frontiersin.org/articles/10.3389/fnana.2019.00098/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.frontiersin.org/articles/10.3389/fnana.2019.00098/pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In biomedical research, cell analysis is important to assess physiological and pathophysiological information. Virtual microscopy offers the unique possibility to study the compositions of tissues at a cellular scale. However, images acquired at such high spatial resolution are massive, contain complex information, and are therefore difficult to analyze automatically. In this article, we address the problem of individualization of size-varying and touching neurons in optical microscopy two-dimensional (2-D) images. Our approach is based on a series of processing steps that incorporate increasingly more information. (1) After a step of segmentation of neuron class using a Random Forest classifier, a novel min-max filter is used to enhance neurons’ centroids and boundaries, enabling the use of region growing process based on a contour-based model to drive it to neuron boundary and achieve individualization of touching neurons. (2) Taking into account size-varying neurons, an adaptive multiscale procedure aiming at individualizing touching neurons is proposed. This protocol was evaluated in 17 major anatomical regions from three NeuN-stained macaque brain sections presenting diverse and comprehensive neuron densities. Qualitative and quantitative analyses demonstrate that the proposed method provides satisfactory results in most regions (e.g., caudate, cortex, subiculum, and putamen) and outperforms a baseline Watershed algorithm. Neuron counts obtained with our method show high correlation with an adapted stereology technique performed by two experts (respectively, 0.983 and 0.975 for the two experts). Neuron diameters obtained with our method ranged between 2 and 28.6 μm, matching values reported in the literature. Further works will aim to evaluate the impact of staining and interindividual variability on our protocol.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="brudfors2019tool" class="col-sm-8"> <div class="title">A tool for super-resolving multimodal clinical MRI</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yael Balbastre</em> ,  Parashkev Nachev ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>arXiv</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1909.01140" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://arxiv.org/pdf/1909.01140.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/brudfors/UniRes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present a tool for resolution recovery in multimodal clinical magnetic resonance imaging (MRI). Such images exhibit great variability, both biological and instrumental. This variability makes automated processing with neuroimaging analysis software very challenging. This leaves intelligence extractable only from large-scale analyses of clinical data untapped, and impedes the introduction of automated predictive systems in clinical care. The tool presented in this paper enables such processing, via inference in a generative model of thick-sliced, multi-contrast MR scans. All model parameters are estimated from the observed data, without the need for manual tuning. The model-driven nature of the approach means that no type of training is needed for applicability to the diversity of MR contrasts present in a clinical context. We show on simulated data that the proposed approach outperforms conventional model-based techniques, and on a large hospital dataset of multimodal MRIs that the tool can successfully super-resolve very thick-sliced images. The implementation is available from https://github.com/brudfors/spm_superres.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MedIA</abbr> </div> <div id="ashburner2019algorithm" class="col-sm-8"> <div class="title">An algorithm for learning shape and appearance models without annotations</div> <div class="author"> <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  Kevin Bronik ,  and  <em>Yael Balbastre</em> </div> <div class="periodical"> <em>Medical image analysis</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/https://arxiv.org/html/1807.10731" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841518305462" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S1361841518305462/pdfft?md5=8e907152c9709f68fdd9b5feca5f1201&amp;pid=1-s2.0-S1361841518305462-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WTCN-computational-anatomy-group/Shape-Appearance-Model" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper presents a framework for automatically learning shape and appearance models for medical (and certain other) images. It is based on the idea that having a more accurate shape and appearance model leads to more accurate image registration, which in turn leads to a more accurate shape and appearance model. This leads naturally to an iterative scheme, which is based on a probabilistic generative model that is fit using Gauss-Newton updates within an EM-like framework. It was developed with the aim of enabling distributed privacy-preserving analysis of brain image data, such that shared information (shape and appearance basis functions) may be passed across sites, whereas latent variables that encode individual images remain secure within each site. These latent variables are proposed as features for privacy-preserving data mining applications.<br><br>The approach is demonstrated qualitatively on the KDEF dataset of 2D face images, showing that it can align images that traditionally require shape and appearance models trained using manually annotated data (manually defined landmarks etc.). It is applied to MNIST dataset of handwritten digits to show its potential for machine learning applications, particularly when training data is limited. The model is able to handle “missing data”, which allows it to be cross-validated according to how well it can predict left-out voxels. The suitability of the derived features for classifying individuals into patient groups was assessed by applying it to a dataset of over 1,900 segmented T1-weighted MR images, which included images from the COBRE and ABIDE datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI-SASHIMI</abbr> </div> <div id="brudfors2019empirical" class="col-sm-8"> <div class="title">Empirical bayesian mixture models for medical image translation</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> ,  Parashkev Nachev ,  and  <em>Yaël Balbastre</em> </div> <div class="periodical"> <em>In Simulation and Synthesis in Medical Imaging: 4th International Workshop, SASHIMI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings 4</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1908.05926" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-32778-1_1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1908.05926" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WTCN-computational-anatomy-group/mb" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Automatically generating one medical imaging modality from another is known as medical image translation, and has numerous interesting applications. This paper presents an interpretable generative modelling approach to medical image translation. By allowing a common model for group-wise normalisation and segmentation of brain scans to handle missing data, the model allows for predicting entirely missing modalities from one, or a few, MR contrasts. Furthermore, the model can be trained on a fairly small number of subjects. The proposed model is validated on three clinically relevant scenarios. Results appear promising and show that a principled, probabilistic model of the relationship between multi-channel signal intensities can be used to infer missing modalities – both MR contrasts and CT images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IPMI</abbr> </div> <div id="brudfors2019nonlinear" class="col-sm-8"> <div class="title">Nonlinear Markov random fields learned via backpropagation</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yaël Balbastre</em> ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Information Processing in Medical Imaging: 26th International Conference, IPMI 2019, Hong Kong, China, June 2–7, 2019, Proceedings 26</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1902.10747" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-20351-1_63" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1902.10747.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Although convolutional neural networks (CNNs) currently dominate competitions on image segmentation, for neuroimaging analysis tasks, more classical generative approaches based on mixture models are still used in practice to parcellate brains. To bridge the gap between the two, in this paper we propose a marriage between a probabilistic generative model, which has been shown to be robust to variability among magnetic resonance (MR) images acquired via different imaging protocols, and a CNN. The link is in the prior distribution over the unknown tissue classes, which are classically modelled using a Markov random field. In this work we model the interactions among neighbouring pixels by a type of recurrent CNN, which can encode more complex spatial interactions. We validate our proposed model on publicly available MR data, from different centres, and show that it generalises across imaging protocols. This result demonstrates a successful and principled inclusion of a CNN in a generative model, which in turn could be adapted by any probabilistic generative approach for image segmentation.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Front.Neurosci.</abbr> </div> <div id="vandenberghe2018voxel" class="col-sm-8"> <div class="title">Voxel-based statistical analysis of 3D immunostained tissue imaging</div> <div class="author"> Michel E Vandenberghe ,  Nicolas Souedet ,  Anne-Sophie Hérard ,  Anne-Marie Ayral ,  Florent Letronne ,  <em>Yaël Balbastre</em> , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Elmahdi Sadouni, Philippe Hantraye, Marc Dhenain, Frédérique Frouin, Jean-Charles Lambert, Thierry Delzescaux' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00754/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.frontiersin.org/articles/10.3389/fnins.2018.00754/pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Recently developed techniques to visualize immunostained tissues in 3D and in large samples have expanded the scope of microscopic investigations at the level of the whole brain. Here, we propose to adapt voxel-based statistical analysis to 3D high-resolution images of the immunostained rodent brain. The proposed approach was first validated with a simulation dataset with known cluster locations. Then, it was applied to characterize the effect of ADAM30, a gene involved in the metabolism of the amyloid precursor protein, in a mouse model of Alzheimer’s disease. This work introduces voxel-based analysis of 3D immunostained microscopic brain images and, therefore, opens the door to localized whole-brain exploratory investigation of pathological markers and cellular alterations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Data Br.</abbr> </div> <div id="balbastre2018validation" class="col-sm-8"> <div class="title">A validation dataset for Macaque brain MRI segmentation</div> <div class="author"> <em>Yaël Balbastre</em> ,  Denis Rivière ,  Nicolas Souedet ,  Clara Fischer ,  Anne-Sophie Hérard ,  Susannah Williams , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Michel E. Vandenberghe, Julien Flament, Romina Aron-Badin, Philippe Hantraye, Jean-François Mangin, Thierry Delzescaux' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Data in Brief</em>, 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S2352340917306042" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://www.sciencedirect.com/science/article/pii/S2352340917306042/pdfft?md5=b1a5535925717903667e9526d585cef9&amp;pid=1-s2.0-S2352340917306042-main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.nitrc.org/projects/mircen_macset" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Data</a> </div> <div class="abstract hidden"> <p>Validation data for segmentation algorithms dedicated to preclinical images is fiercely lacking, especially when compared to the large number of databases of Human brain images and segmentations available to the academic community. Not only is such data essential for validating methods, it is also needed for objectively comparing concurrent algorithms and detect promising paths, as segmentation challenges have shown for clinical images.<br><br>The dataset we present here is a first step in this direction. It comprises 10 T2-weighted MRIs of healthy adult macaque brains, acquired on a 7 T magnet, along with corresponding manual segmentations into 17 brain anatomic labelled regions spread over 5 hierarchical levels based on a previously published macaque atlas (Calabrese et al., 2015) [1].<br><br>By giving access to this unique dataset, we hope to provide a reference needed by the non-human primate imaging community. This dataset was used in an article presenting a new primate brain morphology analysis pipeline, Primatologist (Balbastre et al., 2017) [2]. Data is available through a NITRC repository (https://www.nitrc.org/projects/mircen_macset).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MIUA</abbr> </div> <div id="brudfors2018mri" class="col-sm-8"> <div class="title">MRI super-resolution using multi-channel total variation</div> <div class="author"> <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  <em>Yaël Balbastre</em> ,  Parashkev Nachev ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Medical Image Understanding and Analysis: 22nd Conference, MIUA 2018, Southampton, UK, July 9-11, 2018, Proceedings 22</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1810.03422" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-95921-4_21" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1810.03422.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/brudfors/spm_superres" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.miua2018.soton.ac.uk" class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105" rel="external nofollow noopener" target="_blank">🌟 Best paper award</a> <a class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105">Oral presentation</a> </div> <div class="abstract hidden"> <p>This paper presents a generative model for super-resolution in routine clinical magnetic resonance images (MRI), of arbitrary orientation and contrast. The model recasts the recovery of high resolution images as an inverse problem, in which a forward model simulates the slice-select profile of the MR scanner. The paper introduces a prior based on multi-channel total variation for MRI super-resolution. Bias-variance trade-off is handled by estimating hyper-parameters from the low resolution input scans. The model was validated on a large database of brain images. The validation showed that the model can improve brain segmentation, that it can recover anatomical information between images of different MR contrasts, and that it generalises well to the large variability present in MR images of different subjects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI</abbr> </div> <div id="balbastre2018diffeomorphic" class="col-sm-8"> <div class="title">Diffeomorphic brain shape modelling using Gauss-Newton optimisation</div> <div class="author"> <em>Yaël Balbastre</em> ,  <a href="https://github.com/brudfors" rel="external nofollow noopener" target="_blank">Mikael Brudfors</a> ,  Kevin Bronik ,  and  <a href="https://www.fil.ion.ucl.ac.uk/team/computational-anatomy-team/" rel="external nofollow noopener" target="_blank">John Ashburner</a> </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention–MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part I</em> , 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1806.07109" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-00928-1_97" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1806.07109.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/WTCN-computational-anatomy-group/shape-toolbox" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Shape modelling describes methods aimed at capturing the natural variability of shapes and commonly relies on probabilistic interpretations of dimensionality reduction techniques such as principal component analysis. Due to their computational complexity when dealing with dense deformation models such as diffeomorphisms, previous attempts have focused on explicitly reducing their dimension, diminishing de facto their flexibility and ability to model complex shapes such as brains. In this paper, we present a generative model of shape that allows the covariance structure of deformations to be captured without squashing their domain, resulting in better normalisation. An efficient inference scheme based on Gauss-Newton optimisation is used, which enables processing of 3D neuroimaging data. We trained this algorithm on segmented brains from the OASIS database, generating physiologically meaningful deformation trajectories. To prove the model’s robustness, we applied it to unseen data, which resulted in equivalent fitting scores.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Neuroimage</abbr> </div> <div id="balbastre2017primatologist" class="col-sm-8"> <div class="title">Primatologist: a modular segmentation pipeline for macaque brain morphometry</div> <div class="author"> <em>Yaël Balbastre</em> ,  Denis Rivière ,  Nicolas Souedet ,  Clara Fischer ,  Anne-Sophie Hérard ,  Susannah Williams , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Michel E. Vandenberghe, Julien Flament, Romina Aron-Badin, Philippe Hantraye, Jean-François Mangin, Thierry Delzescaux' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>Neuroimage</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S1053811917307450" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://discovery.ucl.ac.uk/id/eprint/10034944/1/Balbsatre_Accepted__Segmentation_MRI_Macaque_2017.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://brainvisa.info/web/tools.html#primate" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Because they bridge the genetic gap between rodents and humans, non-human primates (NHPs) play a major role in therapy development and evaluation for neurological disorders. However, translational research success from NHPs to patients requires an accurate phenotyping of the models. In patients, magnetic resonance imaging (MRI) combined with automated segmentation methods has offered the unique opportunity to assess in vivo brain morphological changes. Meanwhile, specific challenges caused by brain size and high field contrasts make existing algorithms hard to use routinely in NHPs. To tackle this issue, we propose a complete pipeline, Primatologist, for multi-region segmentation. Tissue segmentation is based on a modular statistical model that includes random field regularization, bias correction and denoising and is optimized by expectation-maximization. To deal with the broad variety of structures with different relaxing times at 7 T, images are segmented into 17 anatomical classes, including subcortical regions. Pre-processing steps insure a good initialization of the parameters and thus the robustness of the pipeline. It is validated on 10 T2-weighted MRIs of healthy macaque brains. Classification scores are compared with those of a non-linear atlas registration, and the impact of each module on classification scores is thoroughly evaluated.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">Thesis</abbr> </div> <div id="balbastre2016developpement" class="col-sm-8"> <div class="title">Développement et validation d’outils pour l’analyse morphologique du cerveau de Macaque</div> <div class="author"> <em>Yaël Balbastre</em> </div> <div class="periodical"> <em>Université Paris Saclay (COmUE)</em> , 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="https://theses.hal.science/tel-01636797v1/file/75568_BALBASTRE_2016_diffusion.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IEEE ICIP</abbr> </div> <div id="you2016automated" class="col-sm-8"> <div class="title">Automated cell individualization and counting in cerebral microscopic images</div> <div class="author"> Zhenzhen You ,  Michel E Vandenberghe ,  <em>Yael Balbastre</em> ,  Nicolas Souedet ,  Philippe Hantraye ,  Caroline Jan , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Anne-Sophie Herard, Thierry Delzescaux' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 2016 IEEE International Conference on Image Processing (ICIP)</em> , 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/7532988/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://hal.science/hal-01539966/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In biomedical research, cell counting is important to assess physiological and pathophysiological information. However, the automated analysis of microscopic images of tissues remains extremely challenging. We propose an automated processing protocol for proper segmentation of individual cells in microscopic images. A Gaussian filter is applied to improve signal to noise ratio (SNR) then an original minmax method is proposed to produce an image in which information describing both cell centers (minima) and boundaries are enhanced. Finally, a contour-based model initialized from minima in the min-max cartography is carried out to achieve cell individualization. This method is evaluated on a NeuN-stained macaque brain section in sub-regions presenting various levels of fraction of neuron surface occupation. Comparison with several methods of reference demonstrates that the performances of our method are superior. A first application to the segmentation of neurons in the hippocampus illustrates the ability of our approach to deal with massive and complex data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">MICCAI-BrainLes</abbr> </div> <div id="balbastre2016quantitative" class="col-sm-8"> <div class="title">A quantitative approach to characterize MR contrasts with histology</div> <div class="author"> <em>Yaël Balbastre</em> ,  Michel E Vandenberghe ,  Anne-Sophie Hérard ,  Pauline Gipchtein ,  Caroline Jan ,  Anselme L Perrier , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Philippe Hantraye, Romina Aron-Badin, Jean-François Mangin, Thierry Delzescaux' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: First International Workshop, Brainles 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Revised Selected Papers 1</em> , 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-30858-6_10" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a class="btn btn-sm z-depth-0" style="color: #f29105; border-color: #f29105">Oral presentation</a> </div> <div class="abstract hidden"> <p>Immunohistochemistry is widely used as a gold standard to inspect tissues, characterize their structure and detect pathological alterations. As such, the joint analysis of histological images and other imaging modalities (MRI, PET) is of major interest to interpret these physical signals and establish their correspondence with the biological constitution of the tissues. However, it is challenging to provide a meaningful characterization of the signal specificity. In this paper, we propose an integrated method to quantitatively evaluate the discriminative power of imaging modalities. This method was validated using a macaque brain dataset containing: 3 immunohistochemically stained and 1 histochemically stained series, 1 photographic volume and 1 in vivo T2 weighted MRI. First, biological regions of interest (ROIs) were automatically delineated from histological sections stained for markers of interest and mapped on the target non-specific modalities through co-registration. These non-overlapping ROIs were considered ground truth for later classification. Voxels were evenly split in training and testing sets for a logistic regression model. The statistical significance of resulting accuracy scores was evaluated through null distribution simulations. Such an approach could be of major interest to assess relevant biological characteristics from various imaging modalities.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IEEE EMBC</abbr> </div> <div id="vandenberghe2015robust" class="col-sm-8"> <div class="title">Robust supervised segmentation of neuropathology whole-slide microscopy images</div> <div class="author"> Michel E Vandenberghe ,  <em>Yaël Balbastre</em> ,  Nicolas Souedet ,  Anne-Sophie Hérard ,  Marc Dhenain ,  Frédérique Frouin , and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Thierry Delzescaux' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em> , 2015 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/7319234/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://hal.science/hal-02155732/file/vandenberghe2015ieeeembc.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Alzheimer’s disease is characterized by brain pathological aggregates such as Aβ plaques and neurofibrillary tangles which trigger neuroinflammation and participate to neuronal loss. Quantification of these pathological markers on histological sections is widely performed to study the disease and to evaluate new therapies. However, segmentation of neuropathology images presents difficulties inherent to histology (presence of debris, tissue folding, non-specific staining) as well as specific challenges (sparse staining, irregular shape of the lesions). Here, we present a supervised classification approach for the robust pixel-level classification of large neuropathology whole slide images. We propose a weighted form of Random Forest in order to fit nonlinear decision boundaries that take into account class imbalance. Both color and texture descriptors were used as predictors and model selection was performed via a leave-one-image-out cross-validation scheme. Our method showed superior results compared to the current state of the art method when applied to the segmentation of Aβ plaques and neurofibrillary tangles in a human brain sample. Furthermore, using parallel computing, our approach easily scales-up to large gigabyte-sized images. To show this, we segmented a whole brain histology dataset of a mouse model of Alzheimer’s disease. This demonstrates our method relevance as a routine tool for whole slide microscopy images analysis in clinical and preclinical research settings.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yael Balbastre. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>